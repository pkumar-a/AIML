{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the dataset\n",
    "data = pd.read_csv(\"pima-indians-diabetes.csv\",header = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pima Indians Diabetes Data Set\n",
      "\n",
      "   0    1   2   3    4     5      6   7  8\n",
      "0  6  148  72  35    0  33.6  0.627  50  1\n",
      "1  1   85  66  29    0  26.6  0.351  31  0\n",
      "2  8  183  64   0    0  23.3  0.672  32  1\n",
      "3  1   89  66  23   94  28.1  0.167  21  0\n",
      "4  0  137  40  35  168  43.1  2.288  33  1\n"
     ]
    }
   ],
   "source": [
    "# Visualize some data samples from the dataset\n",
    "print('Pima Indians Diabetes Data Set\\n')\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Stats for the 7 features over the dataset and the 2 classes {8th column}{diabetic/not-diabetic}\n",
      "\n",
      "                0           1           2           3           4           5  \\\n",
      "count  768.000000  768.000000  768.000000  768.000000  768.000000  768.000000   \n",
      "mean     3.845052  120.894531   69.105469   20.536458   79.799479   31.992578   \n",
      "std      3.369578   31.972618   19.355807   15.952218  115.244002    7.884160   \n",
      "min      0.000000    0.000000    0.000000    0.000000    0.000000    0.000000   \n",
      "25%      1.000000   99.000000   62.000000    0.000000    0.000000   27.300000   \n",
      "50%      3.000000  117.000000   72.000000   23.000000   30.500000   32.000000   \n",
      "75%      6.000000  140.250000   80.000000   32.000000  127.250000   36.600000   \n",
      "max     17.000000  199.000000  122.000000   99.000000  846.000000   67.100000   \n",
      "\n",
      "                6           7           8  \n",
      "count  768.000000  768.000000  768.000000  \n",
      "mean     0.471876   33.240885    0.348958  \n",
      "std      0.331329   11.760232    0.476951  \n",
      "min      0.078000   21.000000    0.000000  \n",
      "25%      0.243750   24.000000    0.000000  \n",
      "50%      0.372500   29.000000    0.000000  \n",
      "75%      0.626250   41.000000    1.000000  \n",
      "max      2.420000   81.000000    1.000000  \n"
     ]
    }
   ],
   "source": [
    "# 8th column is the class label\n",
    "print('\\n\\n\\nStats for the 7 features over the dataset and the 2 classes {8th column}{diabetic/not-diabetic}\\n')\n",
    "print(data.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_TEST_RATIO = 0.8        # 80% training data\n",
    "picker = list(range(data.shape[0]))        # get all indices as a list\n",
    "## sometimes the data is arranged classwise and not randomly\n",
    "## therefore we shuffle the indices\n",
    "random.shuffle(picker)\n",
    "trainMax = int(data.shape[0] * TRAIN_TEST_RATIO)\n",
    "\n",
    "train_features = []\n",
    "test_features = []\n",
    "train_labels = []\n",
    "test_labels = []\n",
    "\n",
    "for pick in picker[:trainMax]:\n",
    "    train_features.append(data.values[pick][:-1])\n",
    "    train_labels.append(int(data.values[pick][-1]))\n",
    "for pick in picker[trainMax:]:\n",
    "    test_features.append(data.values[pick][:-1])\n",
    "    test_labels.append(int(data.values[pick][-1]))\n",
    "\n",
    "train_features = np.array(train_features)\n",
    "test_features = np.array(test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  2.   , 100.   ,  54.   ,  28.   , 105.   ,  37.8  ,   0.498,\n",
       "        24.   ,   0.   ])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.values[pick]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(614, 8) 614 (154, 8) 154\n"
     ]
    }
   ],
   "source": [
    "print(train_features.shape, len(train_labels), test_features.shape, len(test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1: Calculate Prior \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1]\n",
      "[401 213]\n"
     ]
    }
   ],
   "source": [
    "# Get the number of unique classes & corresponding number of elements belonging to each class\n",
    "classes, counts = np.unique(train_labels, return_counts=True)\n",
    "print(classes)\n",
    "print(counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "### I assume my classes are from 0 ... N for some N (Here, we have just 2 classes)\n",
    "num_classes = len(classes)\n",
    "num_feats = train_features.shape[1]  #total number of features\n",
    "total_samples = len(train_labels)    #total number of samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Exercise 1: Find the prior probability of each class as the list `prior`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prior for any class = {number of samples belonging to that class/ total_samples}\n",
    "prior = np.array([ x*1.0/total_samples for x in counts ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.65309446 0.34690554]\n"
     ]
    }
   ],
   "source": [
    "print(prior)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Calculate the mean and variance per feature dimension here \n",
    "### from the training set from samples belonging to each class label.\n",
    "\n",
    "means = np.zeros((num_feats, num_classes)) # every feature, for each class\n",
    "stddev = np.zeros((num_feats, num_classes)) # every feature, for each class\n",
    "\n",
    "# For each class\n",
    "for y in classes: # selecting a class 'y'\n",
    "    pts = train_features[np.where( train_labels == y )[0], :]    # get all samples belonging to 'y'\n",
    "    # For each feature\n",
    "    for i in range(num_feats):\n",
    "        means[i, y] = np.mean(pts[:, i])\n",
    "        stddev[i, y] = np.std(pts[:, i])\n",
    "\n",
    "### This completes the training phase\n",
    "### We know have estimated both the prior probability and the posterior distributions from our training set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Exercise 2: Complete the Gaussian function based on the above equation ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian(x, m, v):\n",
    "    g = np.sqrt(1.0/2*np.pi*v*v)*np.exp( -1.0*(((x - m)/v)**2) )\n",
    "    return g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Find the likelihood for each class 'y', once you have $P(X_{i}|y)$ from Exercise 2 ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_likelihood(point, means, stddev):\n",
    "    \n",
    "    feat_prob = np.zeros((num_feats, num_classes))\n",
    "    for y in classes:\n",
    "        for i in range(num_feats):\n",
    "            feat_prob[i, y] = gaussian(point[i], means[i, y], stddev[i, y]) # get the probability\n",
    "    \n",
    "    likelihood = np.zeros((num_classes, 1)) # likelihood for each class 'y'\n",
    "    for y in classes:\n",
    "        # Take the product of all the feature likelihoods of the class considered\n",
    "        likelihood[y] = np.prod(feat_prob[np.nonzero(feat_prob), y]) # mutliply for each feature 'Xi'\n",
    "    \n",
    "    return likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Predict using Naive Bayes classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = []\n",
    "# For each test sample\n",
    "for i in range(len(test_labels)):\n",
    "    \n",
    "    # Get its likelihood of belong to either class\n",
    "    likelihood = get_likelihood(test_features[i, :], means, stddev)\n",
    "    \n",
    "    # Calculate the approximate posterior = likelihood * prior\n",
    "    approx_posterior = [ np.asscalar(x*y) for x,y in zip(likelihood, prior) ]\n",
    "    #approx because of missing P(X) (constant) in the denominator\n",
    "    \n",
    "    # Make the prediction as that class with the maximum approximate posterior\n",
    "    prediction = np.argmax(approx_posterior)\n",
    "    predictions.append(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy\n",
      "0.6558441558441559\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy\")\n",
    "print(np.mean([x == y for x, y in zip(predictions, test_labels)]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
