{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.7"},"colab":{"name":"Data Augmentation.ipynb","provenance":[],"toc_visible":true}},"cells":[{"cell_type":"markdown","metadata":{"id":"j_E0jt-f6L7B","colab_type":"text"},"source":["# Data Augmentation"]},{"cell_type":"markdown","metadata":{"id":"7nhrpoOL6L7C","colab_type":"text"},"source":["**About the data:** <br>\n","The dataset contains 2 folders: yes and no which contains 253 Brain MRI Images. The folder yes contains 155 Brain MRI Images that are tumorous and the folder no contains 98 Brain MRI Images that are non-tumorous. You can find [here](https://www.kaggle.com/navoneel/brain-mri-images-for-brain-tumor-detection)."]},{"cell_type":"markdown","metadata":{"id":"55ok-noN6L7D","colab_type":"text"},"source":["Since this is a small dataset, I used data augmentation in order to create more images."]},{"cell_type":"markdown","metadata":{"id":"ZREdtjQl6L7E","colab_type":"text"},"source":["Also, we could solve the data imbalance issue (since 61% of the data belongs to the tumorous class) using data augmentation."]},{"cell_type":"markdown","metadata":{"id":"RGRAZ3tD6L7E","colab_type":"text"},"source":["## Import Necessary Modules"]},{"cell_type":"code","metadata":{"id":"vS8mEGpMK5tg","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":122},"executionInfo":{"status":"ok","timestamp":1595918829338,"user_tz":-330,"elapsed":20701,"user":{"displayName":"Manoj Kumar","photoUrl":"","userId":"15297673998707979171"}},"outputId":"dae87e39-c447-4577-86b0-daa4a8989576"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"M4SAGK-a6L7F","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1595918836915,"user_tz":-330,"elapsed":3155,"user":{"displayName":"Manoj Kumar","photoUrl":"","userId":"15297673998707979171"}},"outputId":"c32fe664-39bb-48fe-aa19-795fd4fa642e"},"source":["import tensorflow as tf\n","from keras.preprocessing.image import ImageDataGenerator\n","import cv2\n","import imutils\n","import matplotlib.pyplot as plt\n","from os import listdir\n","import time    \n","\n","%matplotlib inline"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Using TensorFlow backend.\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"F9qsm-FGLPZo","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1595918902919,"user_tz":-330,"elapsed":928,"user":{"displayName":"Manoj Kumar","photoUrl":"","userId":"15297673998707979171"}}},"source":["image_dir=\"/content/drive/My Drive/Colab Notebooks/Brain-Tumor/data/\""],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"PXA3s-FV6L7K","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1595918905393,"user_tz":-330,"elapsed":908,"user":{"displayName":"Manoj Kumar","photoUrl":"","userId":"15297673998707979171"}}},"source":["# Nicely formatted time string\n","def hms_string(sec_elapsed):\n","    h = int(sec_elapsed / (60 * 60))\n","    m = int((sec_elapsed % (60 * 60)) / 60)\n","    s = sec_elapsed % 60\n","    return f\"{h}:{m}:{round(s,1)}\""],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"KfCs3jJf6L7N","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1595919075213,"user_tz":-330,"elapsed":910,"user":{"displayName":"Manoj Kumar","photoUrl":"","userId":"15297673998707979171"}}},"source":["def augment_data(file_dir, n_generated_samples, save_to_dir):\n","    \"\"\"\n","    Arguments:\n","        file_dir: A string representing the directory where images that we want to augment are found.\n","        n_generated_samples: A string representing the number of generated samples using the given image.\n","        save_to_dir: A string representing the directory in which the generated images will be saved.\n","    \"\"\"\n","    \n","    #from keras.preprocessing.image import ImageDataGenerator\n","    #from os import listdir\n","    \n","    data_gen = ImageDataGenerator(rotation_range=10, \n","                                  width_shift_range=0.1, \n","                                  height_shift_range=0.1, \n","                                  shear_range=0.1, \n","                                  brightness_range=(0.3, 1.0),\n","                                  horizontal_flip=True, \n","                                  vertical_flip=True, \n","                                  fill_mode='nearest'\n","                                 )\n","\n","    \n","    for filename in listdir(file_dir):\n","        # load the image\n","        image = cv2.imread(file_dir + '//' + filename)\n","        # reshape the image\n","        image = image.reshape((1,)+image.shape)\n","        # prefix of the names for the generated sampels.\n","        save_prefix = 'aug_' + filename[:-4]\n","        # generate 'n_generated_samples' sample images\n","        i=0\n","        for batch in data_gen.flow(x=image, batch_size=1, save_to_dir=save_to_dir, \n","                                           save_prefix=save_prefix, save_format='jpg'):\n","            i += 1\n","            if i > n_generated_samples:\n","                break"],"execution_count":8,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_msMpwn_6L7Q","colab_type":"text"},"source":["Remember that 61% of the data (155 images) are tumorous. And, 39% of the data (98 images) are non-tumorous.<br>\n","So, in order to balance the data we can generate 9 new images for every image that belongs to 'no' class and 6 images for every image that belongs the 'yes' class.<br>"]},{"cell_type":"code","metadata":{"id":"QcTuu5o_6L7R","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1595919281579,"user_tz":-330,"elapsed":203221,"user":{"displayName":"Manoj Kumar","photoUrl":"","userId":"15297673998707979171"}},"outputId":"8a4b1939-a812-4d49-a4c7-ccd11aed6a9d"},"source":["start_time = time.time()\n","\n","augmented_data_path = '/content/drive/My Drive/Colab Notebooks/Brain-Tumor/new_aug_data/'\n","\n","# augment data for the examples with label equal to 'yes' representing tumurous examples\n","augment_data(file_dir=image_dir+'yes', n_generated_samples=6, save_to_dir=augmented_data_path+'yes')\n","# augment data for the examples with label equal to 'no' representing non-tumurous examples\n","augment_data(file_dir=image_dir+'no', n_generated_samples=9, save_to_dir=augmented_data_path+'no')\n","\n","end_time = time.time()\n","execution_time = (end_time - start_time)\n","print(f\"Elapsed time: {hms_string(execution_time)}\")"],"execution_count":9,"outputs":[{"output_type":"stream","text":["Elapsed time: 0:3:22.2\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"SAgj-xt76L7U","colab_type":"text"},"source":["Let's see how many tumorous and non-tumorous examples after performing data augmentation:"]},{"cell_type":"code","metadata":{"id":"BnlvC9Uh6L7V","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1595919291835,"user_tz":-330,"elapsed":939,"user":{"displayName":"Manoj Kumar","photoUrl":"","userId":"15297673998707979171"}}},"source":["def data_summary(main_path):\n","    \n","    yes_path = main_path+'yes'\n","    no_path = main_path+'no'\n","        \n","    # number of files (images) that are in the the folder named 'yes' that represent tumorous (positive) examples\n","    m_pos = len(listdir(yes_path))\n","    # number of files (images) that are in the the folder named 'no' that represent non-tumorous (negative) examples\n","    m_neg = len(listdir(no_path))\n","    # number of all examples\n","    m = (m_pos+m_neg)\n","    \n","    pos_prec = (m_pos* 100.0)/ m\n","    neg_prec = (m_neg* 100.0)/ m\n","    \n","    print(f\"Number of examples: {m}\")\n","    print(f\"Percentage of positive examples: {pos_prec}%, number of pos examples: {m_pos}\") \n","    print(f\"Percentage of negative examples: {neg_prec}%, number of neg examples: {m_neg}\") "],"execution_count":10,"outputs":[]},{"cell_type":"code","metadata":{"id":"HqHawCJs6L7Y","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":68},"executionInfo":{"status":"ok","timestamp":1595919296168,"user_tz":-330,"elapsed":940,"user":{"displayName":"Manoj Kumar","photoUrl":"","userId":"15297673998707979171"}},"outputId":"8704bb51-751f-4691-c6ca-0bea78790782"},"source":["data_summary(augmented_data_path)"],"execution_count":11,"outputs":[{"output_type":"stream","text":["Number of examples: 2065\n","Percentage of positive examples: 52.54237288135593%, number of pos examples: 1085\n","Percentage of negative examples: 47.45762711864407%, number of neg examples: 980\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"YnF5GmcW6L7c","colab_type":"text"},"source":["That's it for this notebook. Now, we can use the augmented data to train our convolutional neural network."]},{"cell_type":"code","metadata":{"id":"YMOZ00jh6L7c","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]}]}